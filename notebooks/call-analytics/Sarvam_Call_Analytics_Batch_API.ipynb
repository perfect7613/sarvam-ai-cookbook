{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njEMu7DlvMUn"
   },
   "source": [
    "# **Instructions to Keep in Mind Before Running the Notebook:**\n",
    "\n",
    "1. **Get the Subscription Key**:  \n",
    "   Go to [dashboard.sarvam.ai](https://dashboard.sarvam.ai) and copy your **API-Subscription Key**.\n",
    "\n",
    "2. **Upload Files**:  \n",
    "   Use the **Upload** button in the Jupyter notebook to upload the files (e.g., audio files) you want to process.\n",
    "\n",
    "3. **Set File Path**:  \n",
    "   Modify the file path in the code to match the path of your uploaded file, e.g., file_path = '/path/to/your/file.wav'.\n",
    "\n",
    "4. **Set Download Directory**:  \n",
    "   Change the directory path where you want the files to be saved after download, e.g., destination_dir = '/path/to/your/local/directory'.\n",
    "\n",
    "5. **Run the Code**:  \n",
    "   Execute the code with the correct **API key**, **file paths**, and **download directory**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS3e13pwTUPr"
   },
   "source": [
    "### **Note:-**\n",
    "\n",
    "The Batch API currently supports the file duration of 10 minutes. If you have a longer file, then you can try splitting the file into chunks of 10 minutes each. We have provided the code to split the file into chunks at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R6WPZGv5_2_"
   },
   "source": [
    "\n",
    "## 1. Setup and Configuration\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJvMhkZ6vPTb"
   },
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, let's install the required packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fr1wkbGUvHyH",
    "outputId": "5f036f22-3e6d-4794-f1ad-8f7e02b5f9c0"
   },
   "outputs": [],
   "source": [
    "! pip install -Uqq azure-storage-file-datalake aiofiles aiohttp requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6AO6tIgvVIc"
   },
   "source": [
    "Now import the necessary libraries and set up logging:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FFILCZmMvcjm"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiofiles\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from azure.storage.filedatalake.aio import DataLakeDirectoryClient, FileSystemClient\n",
    "from azure.storage.filedatalake import ContentSettings\n",
    "import mimetypes\n",
    "import logging\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "API_SUBSCRIPTION_KEY = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkjNgOTmvgZP"
   },
   "source": [
    "## 2. SarvamClient Class\n",
    "\n",
    "The SarvamClient class handles all Batch operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7yGItkUpvtr7"
   },
   "outputs": [],
   "source": [
    "class SarvamClient:\n",
    "    def __init__(self, url: str):\n",
    "        self.account_url, self.file_system_name, self.directory_name, self.sas_token = (\n",
    "            self._extract_url_components(url)\n",
    "        )\n",
    "        self.lock = asyncio.Lock()\n",
    "        print(f\"Initialized SarvamClient with directory: {self.directory_name}\")\n",
    "\n",
    "    def update_url(self, url: str):\n",
    "        self.account_url, self.file_system_name, self.directory_name, self.sas_token = (\n",
    "            self._extract_url_components(url)\n",
    "        )\n",
    "        print(f\"Updated URL to directory: {self.directory_name}\")\n",
    "\n",
    "    def _extract_url_components(self, url: str):\n",
    "        parsed_url = urlparse(url)\n",
    "        account_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\".replace(\n",
    "            \".blob.\", \".dfs.\"\n",
    "        )\n",
    "        path_components = parsed_url.path.strip(\"/\").split(\"/\")\n",
    "        file_system_name = path_components[0]\n",
    "        directory_name = \"/\".join(path_components[1:])\n",
    "        sas_token = parsed_url.query\n",
    "        return account_url, file_system_name, directory_name, sas_token\n",
    "\n",
    "    async def upload_files(self, local_file_paths, overwrite=True):\n",
    "        print(f\"Starting upload of {len(local_file_paths)} files\")\n",
    "        async with DataLakeDirectoryClient(\n",
    "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
    "            file_system_name=self.file_system_name,\n",
    "            directory_name=self.directory_name,\n",
    "            credential=None,\n",
    "        ) as directory_client:\n",
    "            tasks = []\n",
    "            for path in local_file_paths:\n",
    "                file_name = path.split(\"/\")[-1]\n",
    "                tasks.append(\n",
    "                    self._upload_file(directory_client, path, file_name, overwrite)\n",
    "                )\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            print(\n",
    "                f\"Upload completed for {sum(1 for r in results if not isinstance(r, Exception))} files\"\n",
    "            )\n",
    "\n",
    "    async def _upload_file(\n",
    "        self, directory_client, local_file_path, file_name, overwrite=True\n",
    "    ):\n",
    "        try:\n",
    "            async with aiofiles.open(local_file_path, mode=\"rb\") as file_data:\n",
    "                mime_type = mimetypes.guess_type(local_file_path)[0] or \"audio/wav\"\n",
    "                file_client = directory_client.get_file_client(file_name)\n",
    "                data = await file_data.read()\n",
    "                await file_client.upload_data(\n",
    "                    data,\n",
    "                    overwrite=overwrite,\n",
    "                    content_settings=ContentSettings(content_type=mime_type),\n",
    "                )\n",
    "                print(f\"‚úÖ File uploaded successfully: {file_name}\")\n",
    "                print(f\"   Type: {mime_type}\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Upload failed for {file_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    async def list_files(self):\n",
    "        print(\"\\nüìÇ Listing files in directory...\")\n",
    "        file_names = []\n",
    "        async with FileSystemClient(\n",
    "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
    "            file_system_name=self.file_system_name,\n",
    "            credential=None,\n",
    "        ) as file_system_client:\n",
    "            async for path in file_system_client.get_paths(self.directory_name):\n",
    "                file_name = path.name.split(\"/\")[-1]\n",
    "                async with self.lock:\n",
    "                    file_names.append(file_name)\n",
    "        print(f\"Found {len(file_names)} files:\")\n",
    "        for file in file_names:\n",
    "            print(f\"   üìÑ {file}\")\n",
    "        return file_names\n",
    "\n",
    "    async def download_files(self, file_names, destination_dir):\n",
    "        print(f\"\\n‚¨áÔ∏è Starting download of {len(file_names)} files to {destination_dir}\")\n",
    "        async with DataLakeDirectoryClient(\n",
    "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
    "            file_system_name=self.file_system_name,\n",
    "            directory_name=self.directory_name,\n",
    "            credential=None,\n",
    "        ) as directory_client:\n",
    "            tasks = []\n",
    "            for file_name in file_names:\n",
    "                tasks.append(\n",
    "                    self._download_file(directory_client, file_name, destination_dir)\n",
    "                )\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            print(\n",
    "                f\"Download completed for {sum(1 for r in results if not isinstance(r, Exception))} files\"\n",
    "            )\n",
    "\n",
    "    async def _download_file(self, directory_client, file_name, destination_dir):\n",
    "        try:\n",
    "            file_client = directory_client.get_file_client(file_name)\n",
    "            download_path = f\"{destination_dir}/{file_name}\"\n",
    "            async with aiofiles.open(download_path, mode=\"wb\") as file_data:\n",
    "                stream = await file_client.download_file()\n",
    "                data = await stream.readall()\n",
    "                await file_data.write(data)\n",
    "            print(f\"‚úÖ Downloaded: {file_name} -> {download_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Download failed for {file_name}: {str(e)}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id2zfGrHv--l"
   },
   "source": [
    "## 3. Sarvam AI API Integration\n",
    "\n",
    "These functions handle the Call-Analytics job lifecycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3FxvpEMTwA5P"
   },
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.sarvam.ai/call-analytics/\"\n",
    "\n",
    "\n",
    "async def initialize_job():\n",
    "    print(\"\\nüöÄ Initializing job...\")\n",
    "    url = BASE_URL + \"job/init\"\n",
    "    headers = {\"API-Subscription-Key\": API_SUBSCRIPTION_KEY}\n",
    "    response = requests.post(url, headers=headers)\n",
    "    print(\"\\nInitialize Job Response:\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(\"Response Body:\")\n",
    "    pprint(response.json() if response.status_code == 202 else response.text)\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        return response.json()\n",
    "    return None\n",
    "\n",
    "\n",
    "async def check_job_status(job_id):\n",
    "    print(f\"\\nüîç Checking status for job: {job_id}\")\n",
    "    url = BASE_URL + f\"job/{job_id}/status\"\n",
    "    headers = {\"API-Subscription-Key\": API_SUBSCRIPTION_KEY}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(\"\\nJob Status Response:\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(\"Response Body:\")\n",
    "    pprint(response.json() if response.status_code == 200 else response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    return None\n",
    "\n",
    "\n",
    "async def start_job(job_id):\n",
    "    print(f\"\\n‚ñ∂Ô∏è Starting job: {job_id}\")\n",
    "    url = BASE_URL + \"job\"\n",
    "    headers = {\n",
    "        \"API-Subscription-Key\": API_SUBSCRIPTION_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # JOB PARAMETERS\n",
    "    data = {\n",
    "        \"job_id\": job_id,\n",
    "        \"job_parameters\": {\n",
    "            \"model\": \"saaras:v2\",\n",
    "            \"with_diarization\": True,\n",
    "            \"num_speakers\": 1,\n",
    "            \"questions\": [\n",
    "                {\n",
    "                    \"id\": \"1\",\n",
    "                    \"type\": \"short answer\",\n",
    "                    \"text\": \"what is the sentiment of the user?\",\n",
    "                    \"description\": \"speaker_1 is agent and speaker_0 is user\",\n",
    "                },\n",
    "                {\n",
    "                    \"id\": \"2\",\n",
    "                    \"type\": \"short answer\",\n",
    "                    \"text\": \"what is the summary of the call?\",\n",
    "                    \"description\": \"give crisp executive summary\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "    print(\"\\nRequest Body:\")\n",
    "    pprint(data)\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    print(\"\\nStart Job Response:\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(\"Response Body:\")\n",
    "    pprint(response.json() if response.status_code == 200 else response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAC7-sUgwEYw"
   },
   "source": [
    "## 4. Main Execution Flow\n",
    "\n",
    "Here's the main function that orchestrates the entire process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqagsZaMwE7B",
    "outputId": "f53baca3-6b5b-4c97-c593-bf8a534e4fad"
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    print(\"\\\\n=== Starting Call-Analytics Processing ===\")\n",
    "\n",
    "    # Step 1: Initialize the job\n",
    "    job_info = await initialize_job()\n",
    "    if not job_info:\n",
    "        print(\"‚ùå Job initialization failed\")\n",
    "        return\n",
    "\n",
    "    job_id = job_info[\"job_id\"]\n",
    "    input_storage_path = job_info[\"input_storage_path\"]\n",
    "    output_storage_path = job_info[\"output_storage_path\"]\n",
    "\n",
    "    # Step 2: Upload files\n",
    "    print(f\"\\\\nüì§ Uploading files to input storage: {input_storage_path}\")\n",
    "    client = SarvamClient(input_storage_path)\n",
    "    local_files = [\n",
    "        \"/Users/vinayakgavariya/Downloads/Call-Recording (1).mp3\"\n",
    "    ]  # Replace with your audio files\n",
    "    print(f\"Files to upload: {local_files}\")\n",
    "    await client.upload_files(local_files)\n",
    "\n",
    "    # Step 3: Start the job\n",
    "    job_start_response = await start_job(job_id)\n",
    "    if not job_start_response:\n",
    "        print(\"‚ùå Failed to start job\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Monitor job status\n",
    "    print(\"\\\\n‚è≥ Monitoring job status...\")\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        print(f\"\\\\nStatus check attempt {attempt}\")\n",
    "        job_status = await check_job_status(job_id)\n",
    "        if not job_status:\n",
    "            print(\"‚ùå Failed to get job status\")\n",
    "            break\n",
    "\n",
    "        status = job_status[\"job_state\"]\n",
    "        if status == \"Completed\":\n",
    "            print(\"‚úÖ Job completed successfully!\")\n",
    "            break\n",
    "        elif status == \"Failed\":\n",
    "            print(\"‚ùå Job failed!\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"‚è≥ Current status: {status}\")\n",
    "            await asyncio.sleep(10)\n",
    "        attempt += 1\n",
    "\n",
    "    # Step 5: Download results\n",
    "    if status == \"Completed\":\n",
    "        print(f\"\\nüì• Downloading results from: {output_storage_path}\")\n",
    "        client.update_url(output_storage_path)  # Update URL to the file path\n",
    "\n",
    "        # List all the files you want to download\n",
    "        files = await client.list_files()\n",
    "\n",
    "        # Specify the local destination directory\n",
    "        destination_dir = \"sarvam-ai-cookbook/notebooks/call-analytics/output\"  # Set this to the local path you want\n",
    "\n",
    "        # Make sure the directory exists before downloading\n",
    "        os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "        # Download the files to the local directory\n",
    "        await client.download_files(files, destination_dir=destination_dir)\n",
    "        print(f\"Files have been downloaded to: {destination_dir}\")\n",
    "\n",
    "        print(\"\\\\n=== Processing Complete ===\")\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRmqacY2TUPu"
   },
   "source": [
    "## **For files longer than 10 minutes**\n",
    "\n",
    "### **Define the `split_audio` Function**\n",
    "\n",
    "This function splits an audio file into smaller chunks of a specified duration. This is useful for processing long audio files that exceed the API's input length limit.\n",
    "\n",
    "### Note:-\n",
    "Please modify this code according to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k8IyYt4DTUPu"
   },
   "outputs": [],
   "source": [
    "def split_audio(audio_path, chunk_duration_ms):\n",
    "    \"\"\"\n",
    "    Splits an audio file into smaller chunks of specified duration.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file to be split.\n",
    "        chunk_duration_ms (int): Duration of each chunk in milliseconds.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of AudioSegment objects representing the audio chunks.\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_file(audio_path)  # Load the audio file\n",
    "    chunks = []\n",
    "    if len(audio) > chunk_duration_ms:\n",
    "        # Split the audio into chunks of the specified duration\n",
    "        for i in range(0, len(audio), chunk_duration_ms):\n",
    "            chunks.append(audio[i : i + chunk_duration_ms])\n",
    "    else:\n",
    "        # If the audio is shorter than the chunk duration, use the entire audio\n",
    "        chunks.append(audio)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "plMH7nuuTUPu"
   },
   "outputs": [],
   "source": [
    "def transcribe_audio_chunks(\n",
    "    audio_file_path, api_url, headers, data, chunk_duration_ms=5 * 60 * 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Transcribes audio chunks using the Speech-to-Text API.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        api_url (str): The API endpoint URL for Speech-to-Text.\n",
    "        headers (dict): Headers containing authentication information.\n",
    "        data (dict): Data payload for the transcription API.\n",
    "        chunk_duration_ms (int): Duration of each audio chunk in milliseconds.\n",
    "\n",
    "    Returns:\n",
    "        dict: Collated response containing the transcript.\n",
    "    \"\"\"\n",
    "    # Split the audio into chunks\n",
    "    chunks = split_audio(audio_file_path, chunk_duration_ms)\n",
    "    responses = []  # List to store the transcription results\n",
    "\n",
    "    # Process each chunk\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        # Export the chunk to a BytesIO object (in-memory binary stream)\n",
    "        chunk_buffer = io.BytesIO()\n",
    "        chunk.export(chunk_buffer, format=\"wav\")\n",
    "        chunk_buffer.seek(0)  # Reset the pointer to the start of the stream\n",
    "\n",
    "        # Prepare the file for the API request\n",
    "        files = {\n",
    "            \"file\": (\"/content/SecondCallAnalyticsJob.wav\", chunk_buffer, \"audio/wav\")\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Make the POST request to the API\n",
    "            response = requests.post(api_url, headers=headers, files=files, data=data)\n",
    "            if response.status_code == 200 or response.status_code == 201:\n",
    "                print(f\"Chunk {idx} POST Request Successful!\")\n",
    "                response_data = response.json()\n",
    "                transcript = response_data.get(\"transcript\", \"\")\n",
    "                responses.append({\"transcript\": transcript})\n",
    "            else:\n",
    "                # Handle failed requests\n",
    "                print(\n",
    "                    f\"Chunk {idx} POST Request failed with status code: {response.status_code}\"\n",
    "                )\n",
    "                print(\"Response:\", response.text)\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions during the request\n",
    "            print(f\"Error processing chunk {idx}: {e}\")\n",
    "        finally:\n",
    "            # Ensure the buffer is closed after processing\n",
    "            chunk_buffer.close()\n",
    "\n",
    "    # Collate the transcriptions from all chunks\n",
    "    collated_responses = {\n",
    "        \"collated_transcript\": \" \".join([i[\"transcript\"] for i in responses])\n",
    "    }\n",
    "    return collated_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up1F--FsTUPu"
   },
   "source": [
    "## **Additional Resources**\n",
    "\n",
    "For more details, refer to the official **Saaras API documentation** and join the community for support:\n",
    "\n",
    "- **Documentation**: [docs.sarvam.ai](https://docs.sarvam.ai/)\n",
    "- **Community**: [Join the Discord Community](https://discord.com/invite/T8BGY8TU)\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Notes**\n",
    "\n",
    "- Keep your API key secure.\n",
    "- Use clear audio for best results.\n",
    "- Explore advanced features like diarization and word-level timestamps.\n",
    "\n",
    "**Keep Building!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X3eWbpVldJYl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
